{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons==0.10.0 in c:\\users\\tae\\anaconda3\\lib\\site-packages (0.10.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\tae\\anaconda3\\lib\\site-packages (from tensorflow_addons==0.10.0) (2.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\tae\\Anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_addons==0.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/65464463/importerror-cannot-import-name-keras-tensor-from-tensorflow-python-keras-eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Concatenate, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "RESULT_SAVE_PATH = 'results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception-based 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model():\n",
    "    def inception(filters):\n",
    "        def subnetwork(x):\n",
    "            h1 = Conv2D(filters, (1, 1), padding='same', activation='relu')(x)\n",
    "            h1 = MaxPool2D()(h1)\n",
    "            \n",
    "            h2 = Conv2D(filters // 2, (1, 1), padding='same', activation='relu')(x)\n",
    "            h2 = Conv2D(filters, (3, 3), padding='same', activation='relu')(h2)\n",
    "            h2 = MaxPool2D()(h2)\n",
    "            \n",
    "            h3 = Conv2D(filters // 2, (1, 1), padding='same', activation='relu')(x)\n",
    "            h3 = Conv2D(filters, (5, 5), padding='same', activation='relu')(h3)\n",
    "            h3 = MaxPool2D()(h3)\n",
    "            return Concatenate()([h1, h2, h3])\n",
    "        return subnetwork\n",
    "    \n",
    "    x = tf.keras.Input(shape=(256, 256, 3))\n",
    "    h = inception(16)(x)\n",
    "    h = inception(32)(h)\n",
    "    h = inception(32)(h)\n",
    "    h = inception(32)(h)\n",
    "    h = inception(32)(h)\n",
    "    h = Flatten()(h)\n",
    "    h = Dense(1024, activation='relu')(h)\n",
    "    y = Dense(1, activation='sigmoid')(h)\n",
    "    return tf.keras.Model(inputs=x, outputs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    return tf.image.convert_image_dtype(img, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(img, label):\n",
    "    def flip(x):\n",
    "        x = tf.image.random_flip_left_right(x)\n",
    "        x = tf.image.random_flip_up_down(x)\n",
    "        return x\n",
    "    \n",
    "    def rotate(x):\n",
    "        x = tf.cond(tf.random.uniform(shape=[], minval=0.0, maxval=1.0, dtype=tf.float32) > 0.5,\n",
    "                   lambda: tfa.image.rotate(x,\n",
    "                                       tf.random.uniform(shape=[], minval=0.0, maxval=360.0, dtype=tf.float32),\n",
    "                                       interpolation='BILINEAR'),\n",
    "                   lambda: x)\n",
    "        return x\n",
    "    \n",
    "    def translation(x):\n",
    "        dx = tf.random.uniform(shape=[], minval=-10.0, maxval=10.0, dtype=tf.float32)\n",
    "        dy = tf.random.uniform(shape=[], minval=-10.0, maxval=10.0, dtype=tf.float32)\n",
    "        x = tf.cond(tf.random.uniform(shape=[], minval=0.0, maxval=1.0, dtype=tf.float32) > 0.5,\n",
    "                    lambda: tfa.image.transform(x,\n",
    "                                                [0, 0, dx, 0, 0, dy, 0, 0],\n",
    "                                                interpolation='BILINEAR'),\n",
    "                    lambda: x)\n",
    "        return x\n",
    "    \n",
    "    img = flip(img)\n",
    "    img = rotate(img)\n",
    "    img = translation(img)\n",
    "           \n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecords 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tffiles = glob.glob('tfrecords/*')\n",
    "raw_image_dataset = tf.data.TFRecordDataset(tffiles)\n",
    "\n",
    "image_feature_description = {\n",
    "    'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "def _parse_image_label(parsed_dataset):\n",
    "    return preprocess(tf.image.decode_png(parsed_dataset['image_raw'])), parsed_dataset['label']\n",
    "\n",
    "parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n",
    "dataset = parsed_image_dataset.map(_parse_image_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_size = 0\n",
    "for _ in dataset:\n",
    "    ds_size += 1\n",
    "\n",
    "train_size = int(ds_size * 0.7)\n",
    "\n",
    "ds = dataset.shuffle(ds_size)\n",
    "ds_train = ds.take(train_size).shuffle(1024, reshuffle_each_iteration=True).prefetch(1024).batch(32).map(augmentation)\n",
    "ds_valid = ds.skip(train_size).prefetch(1024).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "55/55 [==============================] - 244s 4s/step - loss: 0.7013 - accuracy: 0.5012 - val_loss: 0.6992 - val_accuracy: 0.4866\n",
      "Epoch 2/1000\n",
      "55/55 [==============================] - 259s 5s/step - loss: 0.6939 - accuracy: 0.5052 - val_loss: 0.6920 - val_accuracy: 0.4933\n",
      "Epoch 3/1000\n",
      "55/55 [==============================] - 238s 4s/step - loss: 0.6875 - accuracy: 0.5109 - val_loss: 0.7144 - val_accuracy: 0.5027\n",
      "Epoch 4/1000\n",
      "55/55 [==============================] - 242s 4s/step - loss: 0.6934 - accuracy: 0.5403 - val_loss: 0.6934 - val_accuracy: 0.5161\n",
      "Epoch 5/1000\n",
      "55/55 [==============================] - 247s 4s/step - loss: 0.6918 - accuracy: 0.5086 - val_loss: 0.6747 - val_accuracy: 0.6210\n",
      "Epoch 6/1000\n",
      "55/55 [==============================] - 247s 4s/step - loss: 0.6922 - accuracy: 0.5351 - val_loss: 0.6670 - val_accuracy: 0.6640\n",
      "Epoch 7/1000\n",
      "55/55 [==============================] - 264s 5s/step - loss: 0.6852 - accuracy: 0.5455 - val_loss: 0.6699 - val_accuracy: 0.6411\n",
      "Epoch 8/1000\n",
      "55/55 [==============================] - 258s 5s/step - loss: 0.6873 - accuracy: 0.5478 - val_loss: 0.6622 - val_accuracy: 0.6707\n",
      "Epoch 9/1000\n",
      "55/55 [==============================] - 279s 5s/step - loss: 0.6820 - accuracy: 0.5271 - val_loss: 0.6495 - val_accuracy: 0.5430\n",
      "Epoch 10/1000\n",
      "55/55 [==============================] - 282s 5s/step - loss: 0.6888 - accuracy: 0.5841 - val_loss: 0.6910 - val_accuracy: 0.5524\n",
      "Epoch 11/1000\n",
      "55/55 [==============================] - 302s 5s/step - loss: 0.6907 - accuracy: 0.5317 - val_loss: 0.6797 - val_accuracy: 0.5820\n",
      "Epoch 12/1000\n",
      "55/55 [==============================] - 308s 6s/step - loss: 0.6830 - accuracy: 0.5570 - val_loss: 0.6375 - val_accuracy: 0.6250\n",
      "Epoch 13/1000\n",
      "55/55 [==============================] - 322s 6s/step - loss: 0.6638 - accuracy: 0.5634 - val_loss: 0.6580 - val_accuracy: 0.4906\n",
      "Epoch 14/1000\n",
      "55/55 [==============================] - 299s 5s/step - loss: 0.6786 - accuracy: 0.5478 - val_loss: 0.6732 - val_accuracy: 0.6331\n",
      "Epoch 15/1000\n",
      "55/55 [==============================] - 286s 5s/step - loss: 0.6694 - accuracy: 0.5674 - val_loss: 0.6083 - val_accuracy: 0.7030\n",
      "Epoch 16/1000\n",
      "13/55 [======>.......................] - ETA: 5:16 - loss: 0.6577 - accuracy: 0.6130"
     ]
    }
   ],
   "source": [
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, verbose=1)\n",
    "history = model.fit(ds_train,\n",
    "                    validation_data=ds_valid,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 결과 Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss, 'ro-')\n",
    "plt.plot(val_loss, 'bo-')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/inception_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
