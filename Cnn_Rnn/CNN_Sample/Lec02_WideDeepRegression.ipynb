{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lec02_WideDeepRegression.ipynb","provenance":[{"file_id":"1xeG2jYT32iXKc76gkrKy9LEGTWV3bGtc","timestamp":1589983984100}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"MPYPTlOubhVd"},"source":["## Codes are adapted from SungKim@HKUST"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aiH1ryROrPbJ","executionInfo":{"status":"ok","timestamp":1619661038603,"user_tz":-540,"elapsed":21910,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}},"outputId":"1ee9b77b-d520-42bd-b193-1dba6296d94c"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WR2-JgxorRjC","executionInfo":{"status":"ok","timestamp":1619661041182,"user_tz":-540,"elapsed":1282,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}},"outputId":"1d28c746-3e0f-48e1-b8cd-b68e1fe06be7"},"source":["cd '/content/drive/MyDrive/ColabNotebooks/Lecture/Pytorch/Codes/'"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/ColabNotebooks/Lecture/Pytorch/Codes\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"m2wLF9BOOM2g"},"source":["## 01. Wide and Deep Model (Diabets_logistic)"]},{"cell_type":"code","metadata":{"id":"8GmQTWVFr-qK"},"source":["import torch\n","from torch.autograd import Variable\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Hg-I4bqsAqW","executionInfo":{"status":"ok","timestamp":1619597463607,"user_tz":-540,"elapsed":591,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}},"outputId":"ab8651b1-f2e8-4d50-d61f-8ecc0f980429"},"source":["xy = np.loadtxt('./data/diabetes.csv.gz', delimiter=',', dtype=np.float32)\n","x_data = Variable(torch.from_numpy(xy[:, 0:-1]))\n","y_data = Variable(torch.from_numpy(xy[:, [-1]]))\n","\n","print(x_data.data.shape)\n","print(y_data.data.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([759, 8])\n","torch.Size([759, 1])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nllZAYaWsDNF"},"source":["class Model(torch.nn.Module):\n","\n","    def __init__(self):\n","        \"\"\"\n","        In the constructor we instantiate two nn.Linear module\n","        \"\"\"\n","        super(Model, self).__init__()\n","        self.l1 = torch.nn.Linear(8, 6)\n","        self.l2 = torch.nn.Linear(6, 4)\n","        self.l3 = torch.nn.Linear(4, 1)\n","\n","        self.sigmoid = torch.nn.Sigmoid()\n","\n","    def forward(self, x):\n","        \"\"\"\n","        In the forward function we accept a Variable of input data and we must return\n","        a Variable of output data. We can use Modules defined in the constructor as\n","        well as arbitrary operators on Variables.\n","        \"\"\"\n","        out1 = self.sigmoid(self.l1(x))\n","        out2 = self.sigmoid(self.l2(out1))\n","        y_pred = self.sigmoid(self.l3(out2))\n","        return y_pred\n","\n","# our model\n","model = Model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r1E9UxaMsFZz","executionInfo":{"status":"ok","timestamp":1619597588665,"user_tz":-540,"elapsed":585,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}},"outputId":"194a48f3-5a56-471e-d09a-ff0c01766a7c"},"source":["# Construct our loss function and an Optimizer. The call to model.parameters()\n","# in the SGD constructor will contain the learnable parameters of the two\n","# nn.Linear modules which are members of the model.\n","\n","criterion = torch.nn.BCELoss(size_average=True)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L5E1JIGHsHic","executionInfo":{"status":"ok","timestamp":1619597637231,"user_tz":-540,"elapsed":961,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}},"outputId":"1f192b72-cd37-421e-ddcb-e6758b35b5f1"},"source":["# Training loop\n","for epoch in range(100):\n","        # Forward pass: Compute predicted y by passing x to the model\n","    y_pred = model(x_data)\n","\n","    # Compute and print loss\n","    loss = criterion(y_pred, y_data)\n","    print(epoch, loss.item())\n","\n","    # Zero gradients, perform a backward pass, and update the weights.\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 0.7568041682243347\n","1 0.7470933794975281\n","2 0.7382279634475708\n","3 0.7301363945007324\n","4 0.7227526903152466\n","5 0.7160158753395081\n","6 0.7098703384399414\n","7 0.7042641639709473\n","8 0.699150562286377\n","9 0.6944860816001892\n","10 0.6902311444282532\n","11 0.6863498091697693\n","12 0.6828088164329529\n","13 0.6795782446861267\n","14 0.676630437374115\n","15 0.6739403605461121\n","16 0.6714852452278137\n","17 0.6692441701889038\n","18 0.6671983003616333\n","19 0.6653302907943726\n","20 0.6636243462562561\n","21 0.6620663404464722\n","22 0.6606431603431702\n","23 0.6593428254127502\n","24 0.6581547260284424\n","25 0.6570687890052795\n","26 0.6560762524604797\n","27 0.6551688313484192\n","28 0.6543391942977905\n","29 0.6535804867744446\n","30 0.6528865694999695\n","31 0.6522517800331116\n","32 0.6516710519790649\n","33 0.6511396765708923\n","34 0.6506533622741699\n","35 0.6502082943916321\n","36 0.6498007774353027\n","37 0.6494278311729431\n","38 0.6490863561630249\n","39 0.6487736105918884\n","40 0.6484870910644531\n","41 0.6482247710227966\n","42 0.647984504699707\n","43 0.647764265537262\n","44 0.6475624442100525\n","45 0.647377610206604\n","46 0.6472081542015076\n","47 0.6470528841018677\n","48 0.646910548210144\n","49 0.6467799544334412\n","50 0.646660327911377\n","51 0.6465506553649902\n","52 0.6464500427246094\n","53 0.6463577747344971\n","54 0.646273136138916\n","55 0.6461954712867737\n","56 0.6461243033409119\n","57 0.646058976650238\n","58 0.645999014377594\n","59 0.6459439992904663\n","60 0.6458934545516968\n","61 0.6458471417427063\n","62 0.6458045840263367\n","63 0.6457656025886536\n","64 0.6457297801971436\n","65 0.645696759223938\n","66 0.6456665396690369\n","67 0.6456387639045715\n","68 0.6456132531166077\n","69 0.6455898880958557\n","70 0.645568311214447\n","71 0.6455485820770264\n","72 0.6455304026603699\n","73 0.6455135941505432\n","74 0.6454983353614807\n","75 0.6454841494560242\n","76 0.6454712152481079\n","77 0.6454592347145081\n","78 0.6454482674598694\n","79 0.6454381942749023\n","80 0.6454287767410278\n","81 0.645420253276825\n","82 0.6454123258590698\n","83 0.6454050540924072\n","84 0.6453983783721924\n","85 0.6453921794891357\n","86 0.6453864574432373\n","87 0.6453811526298523\n","88 0.6453762650489807\n","89 0.6453718543052673\n","90 0.6453676223754883\n","91 0.6453638672828674\n","92 0.6453602910041809\n","93 0.6453570127487183\n","94 0.6453540325164795\n","95 0.6453511714935303\n","96 0.6453484892845154\n","97 0.6453461647033691\n","98 0.6453438997268677\n","99 0.6453418135643005\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uQoffm7IqiAY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YuUo2BJEvzDz"},"source":["## 02. Diabet Classification Using Custom DataLoader"]},{"cell_type":"code","metadata":{"id":"T--364TIwLjx"},"source":["# References\n","# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n","# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n","import torch\n","import numpy as np\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sajsCh4RwLmZ"},"source":["class DiabetesDataset(Dataset):\n","    \"\"\" Diabetes dataset.\"\"\"\n","\n","    # Initialize your data, download, etc.\n","    def __init__(self):\n","        xy = np.loadtxt('./data/diabetes.csv.gz',\n","                        delimiter=',', dtype=np.float32)\n","        self.len = xy.shape[0]\n","        self.x_data = torch.from_numpy(xy[:, 0:-1])\n","        self.y_data = torch.from_numpy(xy[:, [-1]])\n","\n","    def __getitem__(self, index):\n","        return self.x_data[index], self.y_data[index]\n","\n","    def __len__(self):\n","        return self.len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCzMnjexwLpW"},"source":["dataset = DiabetesDataset()\n","train_loader = DataLoader(dataset=dataset,\n","                          batch_size=32,\n","                          shuffle=True,\n","                          num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rh16GsZ-wLsJ"},"source":["class Model(torch.nn.Module):\n","\n","    def __init__(self):\n","        \"\"\"\n","        In the constructor we instantiate two nn.Linear module\n","        \"\"\"\n","        super(Model, self).__init__()\n","        self.l1 = torch.nn.Linear(8, 6)\n","        self.l2 = torch.nn.Linear(6, 4)\n","        self.l3 = torch.nn.Linear(4, 1)\n","\n","        self.sigmoid = torch.nn.Sigmoid()\n","\n","    def forward(self, x):\n","        \"\"\"\n","        In the forward function we accept a Variable of input data and we must return\n","        a Variable of output data. We can use Modules defined in the constructor as\n","        well as arbitrary operators on Variables.\n","        \"\"\"\n","        out1 = self.sigmoid(self.l1(x))\n","        out2 = self.sigmoid(self.l2(out1))\n","        y_pred = self.sigmoid(self.l3(out2))\n","        return y_pred\n","\n","# our model\n","model = Model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3b-Ej1BrwLvJ","executionInfo":{"elapsed":699,"status":"ok","timestamp":1618309401045,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"},"user_tz":-540},"outputId":"906e646f-749c-4883-a123-06102b5dfff4"},"source":["# Construct our loss function and an Optimizer. The call to model.parameters()\n","# in the SGD constructor will contain the learnable parameters of the two\n","# nn.Linear modules which are members of the model.\n","criterion = torch.nn.BCELoss(size_average=True)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"38wIbF3ZwLy7","executionInfo":{"elapsed":1443,"status":"ok","timestamp":1618309404004,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"},"user_tz":-540},"outputId":"9f6d7c65-ac02-4d1c-f859-48f51ee1e915"},"source":["# Training loop\n","for epoch in range(2):\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs\n","        inputs, labels = data\n","\n","        # wrap them in Variable\n","        inputs, labels = Variable(inputs), Variable(labels)\n","\n","        # Forward pass: Compute predicted y by passing x to the model\n","        y_pred = model(inputs)\n","\n","        # Compute and print loss\n","        loss = criterion(y_pred, labels)\n","        print(epoch, i, loss.item())\n","\n","        # Zero gradients, perform a backward pass, and update the weights.\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 0 0.6928312182426453\n","0 1 0.6921122074127197\n","0 2 0.6880536079406738\n","0 3 0.6801295280456543\n","0 4 0.6746836304664612\n","0 5 0.68610018491745\n","0 6 0.6735557913780212\n","0 7 0.6752679347991943\n","0 8 0.6570809483528137\n","0 9 0.6714505553245544\n","0 10 0.6776877641677856\n","0 11 0.6772294044494629\n","0 12 0.6846486926078796\n","0 13 0.6462717652320862\n","0 14 0.6157056093215942\n","0 15 0.6139795184135437\n","0 16 0.6406874656677246\n","0 17 0.627046525478363\n","0 18 0.6877992153167725\n","0 19 0.612067461013794\n","0 20 0.7020623087882996\n","0 21 0.6881701350212097\n","0 22 0.611191987991333\n","0 23 0.6691051721572876\n","1 0 0.6891078352928162\n","1 1 0.6754580736160278\n","1 2 0.6482507586479187\n","1 3 0.6756840348243713\n","1 4 0.647898256778717\n","1 5 0.6615686416625977\n","1 6 0.6612523794174194\n","1 7 0.6472554802894592\n","1 8 0.704450786113739\n","1 9 0.5918007493019104\n","1 10 0.6461097002029419\n","1 11 0.5851393938064575\n","1 12 0.6446921229362488\n","1 13 0.6936346888542175\n","1 14 0.5804551243782043\n","1 15 0.694953441619873\n","1 16 0.5776549577713013\n","1 17 0.6086737513542175\n","1 18 0.6795974969863892\n","1 19 0.6435074210166931\n","1 20 0.5542621612548828\n","1 21 0.5670082569122314\n","1 22 0.781071126461029\n","1 23 0.6714583039283752\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_DG4BnLRqiFc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IO3PzBL27Awr"},"source":["## 03. Softmax Loss"]},{"cell_type":"code","metadata":{"id":"KiyulabEqiHs","executionInfo":{"status":"ok","timestamp":1619660763257,"user_tz":-540,"elapsed":3664,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TYDwlcf_7WVI","executionInfo":{"status":"ok","timestamp":1619660840457,"user_tz":-540,"elapsed":570,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}},"outputId":"8cf245a4-8379-45dc-b475-dfbf9d96308c"},"source":["# Cross entropy example\n","import numpy as np\n","# One hot\n","# 0: 1 0 0\n","# 1: 0 1 0\n","# 2: 0 0 1\n","Y = np.array([1, 0, 0])\n","\n","Y_pred1 = np.array([0.7, 0.2, 0.1])\n","Y_pred2 = np.array([0.1, 0.3, 0.6])\n","print(\"loss1 = \", np.sum(-Y * np.log(Y_pred1)))\n","print(\"loss2 = \", np.sum(-Y * np.log(Y_pred2)))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["loss1 =  0.35667494393873245\n","loss2 =  2.3025850929940455\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2XyK47Zh7btu","executionInfo":{"status":"ok","timestamp":1619660847746,"user_tz":-540,"elapsed":657,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}}},"source":["# Softmax + CrossEntropy (logSoftmax + NLLLoss)\n","loss = nn.CrossEntropyLoss()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJgR4jNf7fhx","executionInfo":{"status":"ok","timestamp":1619660851060,"user_tz":-540,"elapsed":636,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}}},"source":["# target is of size nBatch\n","# each element in target has to have 0 <= value < nClasses (0-2)\n","# Input is class, not one-hot\n","Y = Variable(torch.LongTensor([0]), requires_grad=False)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"yAbsi8wX7jtD","executionInfo":{"status":"ok","timestamp":1619660853951,"user_tz":-540,"elapsed":583,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}}},"source":["# input is of size nBatch x nClasses = 1 x 4\n","# Y_pred are logits (not softmax)\n","Y_pred1 = Variable(torch.Tensor([[2.0, 1.0, 0.1]]))\n","Y_pred2 = Variable(torch.Tensor([[0.5, 2.0, 0.3]]))"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dt_QMCmR7mQH","executionInfo":{"status":"ok","timestamp":1619660858597,"user_tz":-540,"elapsed":568,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}},"outputId":"c82a360c-5863-43d5-dbf6-689b759f79f6"},"source":["l1 = loss(Y_pred1, Y)\n","l2 = loss(Y_pred2, Y)\n","\n","print(\"PyTorch Loss1 = \", l1.data, \"\\nPyTorch Loss2=\", l2.data)\n","\n","print(\"Y_pred1=\", torch.max(Y_pred1.data, 1)[1])\n","print(\"Y_pred2=\", torch.max(Y_pred2.data, 1)[1])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["PyTorch Loss1 =  tensor(0.4170) \n","PyTorch Loss2= tensor(1.8406)\n","Y_pred1= tensor([0])\n","Y_pred2= tensor([1])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LMH4T_b-7pUt","executionInfo":{"status":"ok","timestamp":1619660879697,"user_tz":-540,"elapsed":588,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}}},"source":["# target is of size nBatch\n","# each element in target has to have 0 <= value < nClasses (0-2)\n","# Input is class, not one-hot\n","Y = Variable(torch.LongTensor([2, 0, 1]), requires_grad=False)\n","\n","# input is of size nBatch x nClasses = 2 x 4\n","# Y_pred are logits (not softmax)\n","Y_pred1 = Variable(torch.Tensor([[0.1, 0.2, 0.9],\n","                                 [1.1, 0.1, 0.2],\n","                                 [0.2, 2.1, 0.1]]))\n","\n","\n","Y_pred2 = Variable(torch.Tensor([[0.8, 0.2, 0.3],\n","                                 [0.2, 0.3, 0.5],\n","                                 [0.2, 0.2, 0.5]]))\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yYzH5rM_7zdW","executionInfo":{"status":"ok","timestamp":1619660885905,"user_tz":-540,"elapsed":567,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}},"outputId":"2aa8a469-075d-451e-9f5b-e4521a3beea5"},"source":["l1 = loss(Y_pred1, Y)\n","l2 = loss(Y_pred2, Y)\n","\n","print(\"Batch Loss1 = \", l1.data, \"\\nBatch Loss2=\", l2.data)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Batch Loss1 =  tensor(0.4966) \n","Batch Loss2= tensor(1.2389)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g3DiU2h7qiMr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j9lD4np79TbV"},"source":["## 04.Softmax Classifier Using MNIST\n"]},{"cell_type":"code","metadata":{"id":"ygFt6RQh9Stn","executionInfo":{"status":"ok","timestamp":1619661057704,"user_tz":-540,"elapsed":572,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}}},"source":["# https://github.com/pytorch/examples/blob/master/mnist/main.py\n","from __future__ import print_function\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"p-7I0Hw09ef-","executionInfo":{"status":"ok","timestamp":1619661070215,"user_tz":-540,"elapsed":3106,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}}},"source":["# Training settings\n","batch_size = 64\n","\n","# MNIST Dataset\n","train_dataset = datasets.MNIST(root='./data/',\n","                               train=True,\n","                               transform=transforms.ToTensor(),\n","                               download=True)\n","\n","test_dataset = datasets.MNIST(root='./data/',\n","                              train=False,\n","                              transform=transforms.ToTensor())\n","\n","# Data Loader (Input Pipeline)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"0GExnc789hCo","executionInfo":{"status":"ok","timestamp":1619661187456,"user_tz":-540,"elapsed":596,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}}},"source":["class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.l1 = nn.Linear(784, 520)\n","        self.l2 = nn.Linear(520, 320)\n","        self.l3 = nn.Linear(320, 240)\n","        self.l4 = nn.Linear(240, 120)\n","        self.l5 = nn.Linear(120, 10)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n","        x = F.relu(self.l1(x))\n","        x = F.relu(self.l2(x))\n","        x = F.relu(self.l3(x))\n","        x = F.relu(self.l4(x))\n","        return self.l5(x)\n","\n","\n","model = Net()"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jq_d-tHn9mEQ","executionInfo":{"status":"ok","timestamp":1619661190188,"user_tz":-540,"elapsed":547,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}}},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"fZJH0bac9mGw","executionInfo":{"status":"ok","timestamp":1619661193041,"user_tz":-540,"elapsed":696,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}}},"source":["def train(epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = Variable(data), Variable(target)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 10 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"EEycEey69hFB","executionInfo":{"status":"ok","timestamp":1619661196106,"user_tz":-540,"elapsed":594,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}}},"source":["def test():\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in test_loader:\n","        data, target = Variable(data, volatile=True), Variable(target)\n","        output = model(data)\n","        # sum up batch loss\n","        test_loss += criterion(output, target).item()\n","        # get the index of the max\n","        pred = output.data.max(1, keepdim=True)[1]\n","        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"FBkMRjF-9qMg","executionInfo":{"status":"error","timestamp":1619661266861,"user_tz":-540,"elapsed":69014,"user":{"displayName":"허용석","photoUrl":"","userId":"16468045138349612020"}},"outputId":"2f93aead-0349-4948-a457-cc6f371d72e6"},"source":["for epoch in range(1, 10):\n","    train(epoch)\n","    test()"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305732\n","Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.300231\n","Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.304213\n","Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.305982\n","Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.303980\n","Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.303780\n","Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.297139\n","Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.297724\n","Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.298939\n","Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.302347\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.301841\n","Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.308499\n","Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.292003\n","Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.302925\n","Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.302141\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.297645\n","Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.302949\n","Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.295925\n","Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.301425\n","Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.298281\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.295653\n","Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.297296\n","Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.296264\n","Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.293728\n","Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.297494\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.298529\n","Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.299588\n","Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.294277\n","Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.290950\n","Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.292410\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.287204\n","Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.284209\n","Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.291611\n","Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.293535\n","Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.285053\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.286983\n","Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.294959\n","Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.290991\n","Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.301235\n","Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.285108\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.291100\n","Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.287715\n","Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.290581\n","Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.287939\n","Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.280383\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.289552\n","Train Epoch: 1 [29440/60000 (49%)]\tLoss: 2.278765\n","Train Epoch: 1 [30080/60000 (50%)]\tLoss: 2.281651\n","Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.283448\n","Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.274194\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.282464\n","Train Epoch: 1 [32640/60000 (54%)]\tLoss: 2.273363\n","Train Epoch: 1 [33280/60000 (55%)]\tLoss: 2.271257\n","Train Epoch: 1 [33920/60000 (57%)]\tLoss: 2.268760\n","Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.281601\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.270334\n","Train Epoch: 1 [35840/60000 (60%)]\tLoss: 2.270157\n","Train Epoch: 1 [36480/60000 (61%)]\tLoss: 2.260886\n","Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.268104\n","Train Epoch: 1 [37760/60000 (63%)]\tLoss: 2.268963\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.266908\n","Train Epoch: 1 [39040/60000 (65%)]\tLoss: 2.267474\n","Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.274136\n","Train Epoch: 1 [40320/60000 (67%)]\tLoss: 2.264501\n","Train Epoch: 1 [40960/60000 (68%)]\tLoss: 2.269422\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 2.257936\n","Train Epoch: 1 [42240/60000 (70%)]\tLoss: 2.237661\n","Train Epoch: 1 [42880/60000 (71%)]\tLoss: 2.246546\n","Train Epoch: 1 [43520/60000 (72%)]\tLoss: 2.236971\n","Train Epoch: 1 [44160/60000 (74%)]\tLoss: 2.245896\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.246891\n","Train Epoch: 1 [45440/60000 (76%)]\tLoss: 2.248327\n","Train Epoch: 1 [46080/60000 (77%)]\tLoss: 2.220985\n","Train Epoch: 1 [46720/60000 (78%)]\tLoss: 2.213568\n","Train Epoch: 1 [47360/60000 (79%)]\tLoss: 2.209770\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 2.218960\n","Train Epoch: 1 [48640/60000 (81%)]\tLoss: 2.225792\n","Train Epoch: 1 [49280/60000 (82%)]\tLoss: 2.210039\n","Train Epoch: 1 [49920/60000 (83%)]\tLoss: 2.176830\n","Train Epoch: 1 [50560/60000 (84%)]\tLoss: 2.193978\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.169001\n","Train Epoch: 1 [51840/60000 (86%)]\tLoss: 2.172506\n","Train Epoch: 1 [52480/60000 (87%)]\tLoss: 2.141880\n","Train Epoch: 1 [53120/60000 (88%)]\tLoss: 2.095279\n","Train Epoch: 1 [53760/60000 (90%)]\tLoss: 2.068826\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 2.040878\n","Train Epoch: 1 [55040/60000 (92%)]\tLoss: 2.050348\n","Train Epoch: 1 [55680/60000 (93%)]\tLoss: 2.032695\n","Train Epoch: 1 [56320/60000 (94%)]\tLoss: 1.970722\n","Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.970342\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.967844\n","Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.795839\n","Train Epoch: 1 [58880/60000 (98%)]\tLoss: 1.836142\n","Train Epoch: 1 [59520/60000 (99%)]\tLoss: 1.794833\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 0.0268, Accuracy: 5076/10000 (51%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.749382\n","Train Epoch: 2 [640/60000 (1%)]\tLoss: 1.634339\n","Train Epoch: 2 [1280/60000 (2%)]\tLoss: 1.588006\n","Train Epoch: 2 [1920/60000 (3%)]\tLoss: 1.495038\n","Train Epoch: 2 [2560/60000 (4%)]\tLoss: 1.394175\n","Train Epoch: 2 [3200/60000 (5%)]\tLoss: 1.328347\n","Train Epoch: 2 [3840/60000 (6%)]\tLoss: 1.238955\n","Train Epoch: 2 [4480/60000 (7%)]\tLoss: 1.193183\n","Train Epoch: 2 [5120/60000 (9%)]\tLoss: 1.140040\n","Train Epoch: 2 [5760/60000 (10%)]\tLoss: 1.188824\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.016571\n","Train Epoch: 2 [7040/60000 (12%)]\tLoss: 1.040365\n","Train Epoch: 2 [7680/60000 (13%)]\tLoss: 1.028374\n","Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.910791\n","Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.916772\n","Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.979486\n","Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.988795\n","Train Epoch: 2 [10880/60000 (18%)]\tLoss: 1.025181\n","Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.970402\n","Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.839104\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.880985\n","Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.781218\n","Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.595581\n","Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.788215\n","Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.728916\n","Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.746183\n","Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.774425\n","Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.877109\n","Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.645875\n","Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.926126\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.502976\n","Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.917248\n","Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.705106\n","Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.813923\n","Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.746118\n","Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.736930\n","Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.603039\n","Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.704543\n","Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.681592\n","Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.581234\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.747389\n","Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.579871\n","Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.620261\n","Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.529260\n","Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.548451\n","Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.426546\n","Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.921426\n","Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.709801\n","Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.662988\n","Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.859785\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.817281\n","Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.690565\n","Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.423576\n","Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.451510\n","Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.476620\n","Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.665454\n","Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.775845\n","Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.568879\n","Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.484206\n","Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.477984\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.644487\n","Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.412700\n","Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.535265\n","Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.324761\n","Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.428946\n","Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.768889\n","Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.475763\n","Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.615498\n","Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.432207\n","Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.798422\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.379433\n","Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.528893\n","Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.536456\n","Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.644146\n","Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.620982\n","Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.567497\n","Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.525341\n","Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.654735\n","Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.504029\n","Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.566500\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.466187\n","Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.560507\n","Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.520427\n","Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.700441\n","Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.501188\n","Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.579617\n","Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.414883\n","Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.381098\n","Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.384664\n","Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.420704\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.656602\n","Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.357408\n","Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.537392\n","Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.491266\n","\n","Test set: Average loss: 0.0071, Accuracy: 8654/10000 (87%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.375333\n","Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.386123\n","Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.483367\n","Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.319509\n","Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.249106\n","Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.286624\n","Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.359426\n","Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.475871\n","Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.321774\n","Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.371772\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.508686\n","Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.500829\n","Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.547872\n","Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.470724\n","Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.324753\n","Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.504845\n","Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.319837\n","Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.505041\n","Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.639941\n","Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.485145\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.349240\n","Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.296770\n","Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.397529\n","Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.434260\n","Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.309502\n","Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.416915\n","Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.473960\n","Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.268509\n","Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.443929\n","Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.209790\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.627638\n","Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.400435\n","Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.361971\n","Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.412527\n","Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.462524\n","Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.429848\n","Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.328637\n","Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.284600\n","Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.259662\n","Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.477384\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.356246\n","Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.395005\n","Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.517569\n","Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.186263\n","Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.537830\n","Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.224245\n","Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.289393\n","Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.453935\n","Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.352292\n","Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.319609\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.436966\n","Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.246638\n","Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.347172\n","Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.200979\n","Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.409542\n","Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.263626\n","Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.384090\n","Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.136117\n","Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.277173\n","Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.326185\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.487785\n","Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.301015\n","Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.388637\n","Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.276692\n","Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.229092\n","Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.211268\n","Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.373521\n","Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.349413\n","Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.438454\n","Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.506469\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.470157\n","Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.244488\n","Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.416292\n","Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.247156\n","Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.282647\n","Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.498676\n","Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.387838\n","Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.214820\n","Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.316618\n","Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.314566\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.601593\n","Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.395778\n","Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.619417\n","Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.292302\n","Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.300641\n","Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.747842\n","Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.198513\n","Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.297046\n","Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.356234\n","Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.226896\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.414276\n","Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.437270\n","Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.260989\n","Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.328133\n","\n","Test set: Average loss: 0.0048, Accuracy: 9091/10000 (91%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.288717\n","Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.519343\n","Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.368536\n","Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.338157\n","Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.160748\n","Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.415255\n","Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.453259\n","Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.137226\n","Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.166731\n","Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.578841\n","Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.255983\n","Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.613857\n","Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.312241\n","Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.209544\n","Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.140578\n","Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.320788\n","Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.322334\n","Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.284608\n","Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.153292\n","Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.214368\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.206766\n","Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.248304\n","Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.297624\n","Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.142241\n","Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.123398\n","Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.191232\n","Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.242730\n","Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.123443\n","Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.341291\n","Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.369781\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.250578\n","Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.131187\n","Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.328847\n","Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.308947\n","Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.322008\n","Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.157399\n","Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.309762\n","Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.226328\n","Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.282871\n","Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.431275\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.411721\n","Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.269086\n","Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.304851\n","Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.231021\n","Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.457511\n","Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.343772\n","Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.309144\n","Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.365304\n","Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.187664\n","Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.398314\n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.383918\n","Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.331955\n","Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.344884\n","Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.094523\n","Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.126043\n","Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.593810\n","Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.350311\n","Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.213196\n","Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.213828\n","Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.236867\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.193996\n","Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.221024\n","Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.284680\n","Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.224686\n","Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.238015\n","Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.247428\n","Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.199798\n","Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.297701\n","Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.173348\n","Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.159560\n","Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.291531\n","Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.382244\n","Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.093430\n","Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.155245\n","Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.131513\n","Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.181480\n","Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.168770\n","Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.255079\n","Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.105679\n","Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.215400\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.234522\n","Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.151968\n","Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.312325\n","Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.219913\n","Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.341062\n","Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.354846\n","Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.235654\n","Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.215380\n","Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.207954\n","Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.467146\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.449607\n","Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.146790\n","Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.123072\n","Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.184112\n","\n","Test set: Average loss: 0.0037, Accuracy: 9314/10000 (93%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.131328\n","Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.148771\n","Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.188952\n","Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.101877\n","Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.095037\n","Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.314616\n","Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.133462\n","Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.211955\n","Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.118210\n","Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.209037\n","Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.311408\n","Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.098827\n","Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.373365\n","Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.298920\n","Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.203297\n","Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.223563\n","Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.138641\n","Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.156343\n","Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.084150\n","Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.157794\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.198257\n","Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.089254\n","Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.247382\n","Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.407580\n","Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.264752\n","Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.112799\n","Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.217161\n","Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.284251\n","Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.114652\n","Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.183599\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.191873\n","Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.092585\n","Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.205128\n","Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.149983\n","Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.319721\n","Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.653872\n","Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.263487\n","Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.194362\n","Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.369966\n","Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.252311\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.138638\n","Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.289115\n","Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.243496\n","Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.318514\n","Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.187912\n","Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.317789\n","Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.139290\n","Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.243477\n","Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.179660\n","Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.270157\n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.225619\n","Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.221617\n","Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.142397\n","Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.126215\n","Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.085674\n","Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.187853\n","Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.200966\n","Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.170949\n","Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.138718\n","Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.289301\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.390971\n","Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.293291\n","Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.078492\n","Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.209835\n","Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.220603\n","Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.138271\n","Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.122701\n","Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.349411\n","Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.083528\n","Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.169743\n","Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.152957\n","Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.102277\n","Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.109452\n","Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.110486\n","Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.088392\n","Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.258313\n","Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.173142\n","Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.282026\n","Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.135249\n","Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.195266\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.073654\n","Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.146739\n","Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.214032\n","Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.216057\n","Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.136666\n","Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.219798\n","Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.073586\n","Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.163974\n","Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.130920\n","Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.230893\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.109091\n","Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.141968\n","Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.187878\n","Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.088125\n","\n","Test set: Average loss: 0.0029, Accuracy: 9449/10000 (94%)\n","\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.162170\n","Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.164474\n","Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.210977\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-cc3d32b38928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-1cc2b149ef75>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-698a4ae89fec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten the data (n, 1, 28, 28)-> (n, 784)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"Jd0xQk0z9ekn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YCIUtpUlBoE0"},"source":["## 03. Toy Inception MNIST"]},{"cell_type":"code","metadata":{"id":"Ge9Fla60BihQ"},"source":["# https://github.com/pytorch/examples/blob/master/mnist/main.py\n","from __future__ import print_function\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7RSCPF0BBij1"},"source":["# Training settings\n","batch_size = 64\n","\n","# MNIST Dataset\n","train_dataset = datasets.MNIST(root='./data/',\n","                               train=True,\n","                               transform=transforms.ToTensor(),\n","                               download=True)\n","\n","test_dataset = datasets.MNIST(root='./data/',\n","                              train=False,\n","                              transform=transforms.ToTensor())\n","\n","# Data Loader (Input Pipeline)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D3UTib-6BylC"},"source":["class InceptionA(nn.Module):\n","\n","    def __init__(self, in_channels):\n","        super(InceptionA, self).__init__()\n","        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n","\n","        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n","        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n","\n","        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n","        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n","        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n","\n","        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n","\n","    def forward(self, x):\n","        branch1x1 = self.branch1x1(x)\n","\n","        branch5x5 = self.branch5x5_1(x)\n","        branch5x5 = self.branch5x5_2(branch5x5)\n","\n","        branch3x3dbl = self.branch3x3dbl_1(x)\n","        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n","        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n","\n","        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n","        branch_pool = self.branch_pool(branch_pool)\n","\n","        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n","        return torch.cat(outputs, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhA6o_rVByoc"},"source":["class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n","        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n","\n","        self.incept1 = InceptionA(in_channels=10)\n","        self.incept2 = InceptionA(in_channels=20)\n","\n","        self.mp = nn.MaxPool2d(2)\n","        self.fc = nn.Linear(1408, 10)\n","\n","    def forward(self, x):\n","        in_size = x.size(0)\n","        x = F.relu(self.mp(self.conv1(x)))\n","        x = self.incept1(x)\n","        x = F.relu(self.mp(self.conv2(x)))\n","        x = self.incept2(x)\n","        x = x.view(in_size, -1)  # flatten the tensor\n","        x = self.fc(x)\n","        return F.log_softmax(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aEyjTSb-Byq0"},"source":["model = Net()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ziWS8JL6B8WD"},"source":["def train(epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = Variable(data), Variable(target)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 10 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xWZb8OioB8Yi"},"source":["def test():\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in test_loader:\n","        data, target = Variable(data, volatile=True), Variable(target)\n","        output = model(data)\n","        # sum up batch loss\n","        test_loss += F.nll_loss(output, target, size_average=False).item()\n","        # get the index of the max log-probability\n","        pred = output.data.max(1, keepdim=True)[1]\n","        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"jhR86b-iCDJ-","outputId":"978b5492-7fe2-4b1c-954c-80b53f08d050"},"source":["for epoch in range(1, 10):\n","    train(epoch)\n","    test()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298459\n","Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.302282\n","Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.314808\n","Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.313843\n","Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.307911\n","Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.301989\n","Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.292517\n","Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.291605\n","Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.301653\n","Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.293882\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.293476\n","Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.288318\n","Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.307489\n","Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.282997\n","Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.300429\n","Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.278991\n","Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.284487\n","Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.271263\n","Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.272692\n","Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.256499\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.247248\n","Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.219682\n","Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.218287\n","Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.199482\n","Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.072562\n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.018283\n","Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.888470\n","Train Epoch: 1 [17280/60000 (29%)]\tLoss: 1.645551\n","Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.340695\n","Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.054764\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.061440\n","Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.708808\n","Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.811880\n","Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.733036\n","Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.503645\n","Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.691997\n","Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.785598\n","Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.522545\n","Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.401532\n","Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.630583\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.699138\n","Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.406982\n","Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.309511\n","Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.654108\n","Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.346521\n","Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.403801\n","Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.409148\n","Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.310570\n","Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.369544\n","Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.326941\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.406885\n","Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.450345\n","Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.314456\n","Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.545085\n","Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.463935\n","Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.441519\n","Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.311745\n","Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.264772\n","Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.340990\n","Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.511739\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.370371\n","Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.217039\n","Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.291192\n","Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.268370\n","Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.290411\n","Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.244508\n","Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.371804\n","Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.264199\n","Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.092484\n","Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.421108\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.278990\n","Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.351037\n","Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.358358\n","Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.330466\n","Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.379456\n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.236032\n","Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.219422\n","Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.173511\n","Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.307927\n","Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.441642\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.190848\n","Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.349796\n","Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.333977\n","Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.300917\n","Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.181879\n","Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.231879\n","Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.267939\n","Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.222201\n","Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.247269\n","Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.274390\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.129102\n","Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.319077\n","Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.213508\n","Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.375303\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  \n","/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 0.1995, Accuracy: 9402/10000 (94%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.258538\n","Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.217049\n","Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.169010\n","Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.160412\n","Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.413642\n","Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.295826\n","Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.261276\n","Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.092610\n","Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.242168\n","Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.269855\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.191170\n","Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.146693\n","Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.315070\n","Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.197020\n","Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.235060\n","Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.152280\n","Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.170992\n","Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.221109\n","Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.164026\n","Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.068481\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.227278\n","Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.186990\n","Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.070408\n","Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.054403\n","Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.130806\n","Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.146219\n","Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.068909\n","Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.090442\n","Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.097665\n","Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.156443\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.238270\n","Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.282475\n","Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.075664\n","Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.118699\n","Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.116094\n","Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.295763\n","Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.090409\n","Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.083746\n","Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.155618\n","Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.126978\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.150365\n","Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.161991\n","Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.207581\n","Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.198018\n","Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.152199\n","Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.081553\n","Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.158769\n","Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.118967\n","Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.286443\n","Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.228006\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.193929\n","Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.061845\n","Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.199084\n","Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.054408\n","Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.171303\n","Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.065156\n","Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.119923\n","Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.127407\n","Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.121178\n","Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.109711\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.133274\n","Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.272783\n","Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.238547\n","Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.109010\n","Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.251755\n","Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.185380\n","Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.152634\n","Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.112453\n","Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.052718\n","Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.064517\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.171271\n","Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.049186\n","Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.142638\n","Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.165405\n","Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.190254\n","Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.143566\n","Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.082231\n","Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.166204\n","Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.157046\n","Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.123517\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.281554\n","Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.097073\n","Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.117904\n","Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.108541\n","Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.087001\n","Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.079952\n","Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.159783\n","Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.030979\n","Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.129183\n","Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.270887\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.094167\n","Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.139767\n","Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.175420\n","Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.133328\n","\n","Test set: Average loss: 0.0993, Accuracy: 9669/10000 (97%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.170648\n","Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.138560\n","Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.086426\n","Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.211556\n","Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.107071\n","Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.356113\n","Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.062097\n","Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.201259\n","Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.106286\n","Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.107792\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.103604\n","Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.199347\n","Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.062851\n","Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.143251\n","Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.084281\n","Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.051504\n","Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.026430\n","Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.113778\n","Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.129343\n","Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.098869\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.240272\n","Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.037645\n","Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.088315\n","Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.148072\n","Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.127308\n","Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.033290\n","Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.089775\n","Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.088010\n","Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.160561\n","Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.144199\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.094785\n","Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.140886\n","Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.065263\n","Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.060500\n","Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.122892\n","Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.090418\n","Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.095281\n","Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.108139\n","Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.141379\n","Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.069724\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.100991\n","Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.034085\n","Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.107376\n","Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.117907\n","Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.036076\n","Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.176281\n","Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.075367\n","Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.123179\n","Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.073479\n","Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.123025\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.176704\n","Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.020024\n","Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.078683\n","Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.205316\n","Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.099726\n","Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.083663\n","Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.243602\n","Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.151551\n","Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.091908\n","Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.032886\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.211398\n","Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.042125\n","Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.051550\n","Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.117734\n","Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.027261\n","Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.069006\n","Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.163546\n","Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.101421\n","Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.082957\n","Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.134693\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.101315\n","Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.013861\n","Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.140742\n","Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.101273\n","Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.115871\n","Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.076575\n","Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.102614\n","Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.056085\n","Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.151927\n","Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.042757\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.132225\n","Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.129050\n","Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.069755\n","Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.025066\n","Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.030698\n","Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.128241\n","Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.042358\n","Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.172904\n","Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.202946\n","Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.024921\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.034314\n","Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.093986\n","Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.040812\n","Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.275769\n","\n","Test set: Average loss: 0.0781, Accuracy: 9736/10000 (97%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.115700\n","Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.159891\n","Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.084157\n","Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.082409\n","Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.114534\n","Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.168364\n","Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.240476\n","Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.051503\n","Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.156359\n","Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.041403\n","Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.107722\n","Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.006747\n","Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.160790\n","Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.120371\n","Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.040461\n","Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.071576\n","Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.028960\n","Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.104348\n","Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.081743\n","Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.168133\n","Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.080409\n","Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.196245\n","Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.054231\n","Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.080403\n","Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.131812\n","Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.084675\n","Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.094286\n","Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.076001\n","Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.146797\n","Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.032616\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.267053\n","Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.022118\n","Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.100542\n","Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.241978\n","Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.067037\n","Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.134078\n","Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.151388\n","Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.064772\n","Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.220536\n","Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.142012\n","Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.077108\n","Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.059847\n","Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.032835\n","Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.047832\n","Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.035602\n","Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.194868\n","Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.153751\n","Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.032234\n","Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.078801\n","Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.036398\n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.159791\n","Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.011107\n","Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.067687\n","Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.076065\n","Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.042721\n","Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.012607\n","Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.017709\n","Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.041356\n","Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.044725\n","Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.123111\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.065012\n","Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.091001\n","Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.083161\n","Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.093827\n","Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.052253\n","Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.121559\n","Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.035372\n","Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.169356\n","Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.102895\n","Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.056024\n","Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.159188\n","Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.042253\n","Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.054006\n","Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.078829\n","Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.076270\n","Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.052707\n","Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.027666\n","Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.147481\n","Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.102105\n","Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.012875\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.106190\n","Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.085543\n","Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.043952\n","Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.189746\n","Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.280306\n","Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.064627\n","Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.056057\n","Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.076273\n","Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.075768\n","Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.107013\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.028665\n","Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.021938\n","Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.020106\n","Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.027112\n","\n","Test set: Average loss: 0.0685, Accuracy: 9784/10000 (98%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.057820\n","Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.165115\n","Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.037470\n","Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.040103\n","Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.020856\n","Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.048201\n","Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.017258\n","Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.122846\n","Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.114993\n","Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.046604\n","Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.322262\n","Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.059720\n","Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.043759\n","Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.024284\n","Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.030404\n","Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.019994\n","Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.110568\n","Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.068662\n","Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.021905\n","Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.054378\n","Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.017373\n","Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.156619\n","Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.070435\n","Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.080143\n","Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.038655\n","Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.088442\n","Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.124663\n","Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.047805\n","Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.045785\n","Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.022912\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.056519\n","Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.015752\n","Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.040237\n","Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.037050\n","Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.046490\n","Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.072268\n","Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.189665\n","Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.012553\n","Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.061441\n","Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.057284\n","Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.076897\n","Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.070235\n","Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.032040\n","Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.016338\n","Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.047734\n","Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.014042\n","Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.098317\n","Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.089698\n","Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.241461\n","Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.022819\n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.078164\n","Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.024222\n","Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.159130\n","Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.095266\n","Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.066586\n","Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.063557\n","Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.021232\n","Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.073478\n","Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.238276\n","Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.019604\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.035445\n","Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.089762\n","Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.118003\n","Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.076584\n","Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.047040\n","Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.027693\n","Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.026145\n","Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.150196\n","Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.013808\n","Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.173011\n","Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.132879\n","Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.054927\n","Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.042189\n","Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.055055\n","Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.031637\n","Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.133017\n","Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.055436\n","Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.026962\n","Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.095666\n","Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.016448\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.202858\n","Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.060639\n","Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.061898\n","Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.012719\n","Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.040043\n","Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.175558\n","Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.079903\n","Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.012311\n","Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.056584\n","Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.034126\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.022545\n","Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.087674\n","Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.378345\n","Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.076483\n","\n","Test set: Average loss: 0.0613, Accuracy: 9810/10000 (98%)\n","\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.164202\n","Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.102500\n","Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.017067\n","Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.047881\n","Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.066474\n","Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.100150\n","Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.183769\n","Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.086968\n","Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.090347\n","Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.084641\n","Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.172917\n","Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.145761\n","Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.032565\n","Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.029026\n","Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.042554\n","Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.007536\n","Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.035639\n","Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.146887\n","Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.261875\n","Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.302498\n","Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.030474\n","Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.081499\n","Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.019037\n","Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.018738\n","Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.066420\n","Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.038974\n","Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.022689\n","Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.011074\n","Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.128709\n","Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.032931\n","Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.032781\n","Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.028977\n","Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.088064\n","Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.074526\n","Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.110589\n","Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.018449\n","Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.078063\n","Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.191684\n","Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.059489\n","Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.040009\n","Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.131528\n","Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.067261\n","Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.103424\n","Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.016754\n","Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.101211\n","Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.043693\n","Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.021884\n","Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.086163\n","Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.069005\n","Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.063699\n","Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.039485\n","Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.164711\n","Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.015756\n","Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.043159\n","Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.020043\n","Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.024961\n","Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.029701\n","Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.088259\n","Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.018618\n","Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.092008\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.042079\n","Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.097001\n","Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.062448\n","Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.064730\n","Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.035056\n","Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.084534\n","Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.143975\n","Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.082114\n","Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.037576\n","Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.012485\n","Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.046472\n","Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.210724\n","Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.026473\n","Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.081646\n","Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.074322\n","Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.087720\n","Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.070669\n","Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.073368\n","Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.125411\n","Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.080530\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.072058\n","Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.081103\n","Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.052362\n","Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.059940\n","Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.024723\n","Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.048846\n","Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.035134\n","Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.034627\n","Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.096543\n","Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.025712\n","Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.037191\n","Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.248727\n","Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.055848\n","Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.138423\n","\n","Test set: Average loss: 0.0587, Accuracy: 9808/10000 (98%)\n","\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.101815\n","Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.074411\n","Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.019099\n","Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.027092\n","Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.028539\n","Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.055592\n","Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.079528\n","Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.019371\n","Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.058734\n","Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.075036\n","Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.132473\n","Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.043093\n","Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.015524\n","Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.149736\n","Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.201630\n","Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.012691\n","Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.105585\n","Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.062413\n","Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.032532\n","Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.062707\n","Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.086375\n","Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.036326\n","Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.125870\n","Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.016852\n","Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.093513\n","Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.091858\n","Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.115798\n","Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.035243\n","Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.224536\n","Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.013099\n","Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.013542\n","Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.054088\n","Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.038775\n","Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.031732\n","Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.009666\n","Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.060248\n","Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.036089\n","Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.044971\n","Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.010181\n","Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.128735\n","Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.025630\n","Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.088048\n","Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.026749\n","Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.116285\n","Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.152289\n","Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.048042\n","Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.125752\n","Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.029141\n","Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.058612\n","Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.043716\n","Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.008144\n","Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.012084\n","Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.005961\n","Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.011044\n","Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.030894\n","Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.172793\n","Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.014209\n","Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.106960\n","Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.017379\n","Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.137698\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.120515\n","Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.047227\n","Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.116516\n","Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.135623\n","Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.032820\n","Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.025315\n","Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.011435\n","Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.168159\n","Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.006209\n","Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.194949\n","Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.091509\n","Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.019304\n","Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.077129\n","Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.041079\n","Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.081136\n","Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.025167\n","Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.082258\n","Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.152403\n","Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.102351\n","Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.034492\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.214477\n","Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.083395\n","Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.069079\n","Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.023391\n","Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.008320\n","Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.109958\n","Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.109407\n","Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.010480\n","Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.056135\n","Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.055276\n","Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.011841\n","Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.066476\n","Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.083749\n","Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.041710\n","\n","Test set: Average loss: 0.0536, Accuracy: 9825/10000 (98%)\n","\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.078515\n","Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.111863\n","Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.040148\n","Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.034402\n","Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.133246\n","Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.135164\n","Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.044358\n","Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.010112\n","Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.016614\n","Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.011208\n","Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.023697\n","Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.034444\n","Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.121258\n","Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.073405\n","Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.020770\n","Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.071878\n","Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.028991\n","Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.060369\n","Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.150891\n","Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.028589\n","Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.039725\n","Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.055759\n","Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.035310\n","Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.037557\n","Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.014804\n","Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.163907\n","Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.043898\n","Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.020074\n","Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.064692\n","Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.144497\n","Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.067901\n","Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.020898\n","Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.038207\n","Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.026312\n","Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.059740\n","Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.137592\n","Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.033961\n","Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.043845\n","Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.034381\n","Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.045425\n","Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.118250\n","Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.079885\n","Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.126043\n","Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.024484\n","Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.029506\n","Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.007451\n","Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.050132\n","Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.047755\n","Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.035139\n","Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.132446\n","Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.014794\n","Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.093465\n","Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.074156\n","Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.034883\n","Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.202094\n","Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.085357\n","Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.122728\n","Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.024437\n","Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.011046\n","Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.014864\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.022740\n","Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.011270\n","Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.050693\n","Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.043158\n","Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.027648\n","Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.087456\n","Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.017176\n","Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.028354\n","Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.151460\n","Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.045556\n","Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.074245\n","Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.045500\n","Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.062744\n","Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.111121\n","Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.005491\n","Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.029665\n","Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.119619\n","Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.003355\n","Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.065596\n","Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.084657\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.082757\n","Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.053398\n","Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.010624\n","Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.041479\n","Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.083432\n","Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.049943\n","Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.081266\n","Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.005326\n","Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.020565\n","Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.109341\n","Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.107367\n","Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.051295\n","Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.016565\n","Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.067223\n","\n","Test set: Average loss: 0.0503, Accuracy: 9834/10000 (98%)\n","\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.103428\n","Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.090951\n","Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.022459\n","Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.101653\n","Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.111158\n","Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.039981\n","Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.072780\n","Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.003673\n","Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.162489\n","Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.058297\n","Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.025645\n","Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.048173\n","Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.036089\n","Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.087676\n","Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.054269\n","Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.028877\n","Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.135258\n","Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.037477\n","Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.011907\n","Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.083992\n","Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.023377\n","Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.028539\n","Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.036602\n","Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.028472\n","Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.004973\n","Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.013079\n","Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.074352\n","Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.043865\n","Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.051425\n","Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.098851\n","Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.011950\n","Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.015249\n","Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.072036\n","Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.005156\n","Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.040983\n","Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.019498\n","Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.005670\n","Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.034260\n","Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.034565\n","Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.060383\n","Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.051681\n","Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.013333\n","Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.038392\n","Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.019739\n","Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.076064\n","Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.099842\n","Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.079739\n","Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.007881\n","Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.048480\n","Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.006277\n","Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.048086\n","Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.017888\n","Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.007429\n","Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.090440\n","Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.042577\n","Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.010921\n","Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.027236\n","Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.028643\n","Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.131466\n","Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.061158\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.040817\n","Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.051448\n","Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.022467\n","Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.021955\n","Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.026944\n","Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.049203\n","Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.020736\n","Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.015777\n","Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.066339\n","Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.008608\n","Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.014566\n","Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.022749\n","Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.100258\n","Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.350156\n","Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.009679\n","Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.041543\n","Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.125541\n","Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.013950\n","Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.022698\n","Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.034781\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.074877\n","Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.085804\n","Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.011510\n","Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.077823\n","Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.045154\n","Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.018423\n","Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.013129\n","Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.083295\n","Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.058163\n","Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.046699\n","Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.204624\n","Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.025385\n","Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.033146\n","Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.007377\n","\n","Test set: Average loss: 0.0558, Accuracy: 9822/10000 (98%)\n","\n"],"name":"stdout"}]}]}