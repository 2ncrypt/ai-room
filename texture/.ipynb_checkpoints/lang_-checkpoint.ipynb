{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a47000ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Martix is everywhere its all around us, here envn in this room.', 'you can see it out your window or on your television.', 'You fell it when you go to work, or go ro church or pay your taxes.']\n",
      "<class 'list'> 15\n",
      "['The', 'Martix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'envn', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "text_sample = '''The Martix is everywhere its all around us, here envn in this room. you can see it out your window or on your television. You fell it when you go to work, or go ro church or pay your taxes.'''\n",
    "\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)\n",
    "\n",
    "sentence = 'The Martix is everywhere its all around us, here envn in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e525a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print(len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3780edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d8c7dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['martix', 'everywhere', 'around', 'us', ',', 'envn', 'room', '.'], ['see', 'window', 'television', '.'], ['fell', 'go', 'work', ',', 'go', 'ro', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "for sentence in word_tokens:\n",
    "    filterd_words=[]\n",
    "    for word in sentence:\n",
    "        word= word.lower()\n",
    "        if word not in stopwords:\n",
    "            filterd_words.append(word)\n",
    "    all_tokens.append(filterd_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ae04112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "98f92a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83dfc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eafd670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
